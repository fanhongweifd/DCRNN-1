{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is developed to trace the graph convolution tensorflow function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "from lib import utils\n",
    "\n",
    "\n",
    "class DCGRUCell(RNNCell):\n",
    "    \"\"\"Graph Convolution Gated Recurrent Unit cell.\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, num_units, adj_mx, max_diffusion_step, num_nodes, num_proj=None,\n",
    "                 activation=tf.nn.tanh, reuse=None, filter_type=\"laplacian\", use_gc_for_ru=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param num_units:\n",
    "        :param adj_mx:\n",
    "        :param max_diffusion_step:\n",
    "        :param num_nodes:\n",
    "        :param input_size:\n",
    "        :param num_proj:\n",
    "        :param activation:\n",
    "        :param reuse:\n",
    "        :param filter_type: \"laplacian\", \"random_walk\", \"dual_random_walk\".\n",
    "        :param use_gc_for_ru: whether to use Graph convolution to calculate the reset and update gates.\n",
    "        \"\"\"\n",
    "        super(DCGRUCell, self).__init__(_reuse=reuse)\n",
    "        self._activation = activation\n",
    "        self._num_nodes = num_nodes\n",
    "        self._num_proj = num_proj\n",
    "        self._num_units = num_units\n",
    "        self._max_diffusion_step = max_diffusion_step\n",
    "        self._supports = []\n",
    "        self._use_gc_for_ru = use_gc_for_ru\n",
    "        supports = []\n",
    "        if filter_type == \"laplacian\":\n",
    "            supports.append(utils.calculate_scaled_laplacian(adj_mx, lambda_max=None))\n",
    "        elif filter_type == \"random_walk\":\n",
    "            supports.append(utils.calculate_random_walk_matrix(adj_mx).T)\n",
    "        elif filter_type == \"dual_random_walk\":\n",
    "            supports.append(utils.calculate_random_walk_matrix(adj_mx).T)\n",
    "            supports.append(utils.calculate_random_walk_matrix(adj_mx.T).T)\n",
    "        else:\n",
    "            supports.append(utils.calculate_scaled_laplacian(adj_mx))\n",
    "        for support in supports:\n",
    "            self._supports.append(self._build_sparse_matrix(support))\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_nodes * self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        output_size = self._num_nodes * self._num_units\n",
    "        if self._num_proj is not None:\n",
    "            output_size = self._num_nodes * self._num_proj\n",
    "        return output_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Gated recurrent unit (GRU) with Graph Convolution.\n",
    "        :param inputs: (B, num_nodes * input_dim)\n",
    "\n",
    "        :return\n",
    "        - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n",
    "        - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
    "            the arity and shapes of `state`\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(scope or \"dcgru_cell\"):\n",
    "            with tf.variable_scope(\"gates\"):  # Reset gate and update gate.\n",
    "                output_size = 2 * self._num_units\n",
    "                # We start with bias of 1.0 to not reset and not update.\n",
    "                if self._use_gc_for_ru:\n",
    "                    fn = self._gconv\n",
    "                else:\n",
    "                    fn = self._fc\n",
    "                value = tf.nn.sigmoid(fn(inputs, state, output_size, bias_start=1.0))\n",
    "                value = tf.reshape(value, (-1, self._num_nodes, output_size))\n",
    "                r, u = tf.split(value=value, num_or_size_splits=2, axis=-1)\n",
    "                r = tf.reshape(r, (-1, self._num_nodes * self._num_units))\n",
    "                u = tf.reshape(u, (-1, self._num_nodes * self._num_units))\n",
    "            with tf.variable_scope(\"candidate\"):\n",
    "                c = self._gconv(inputs, r * state, self._num_units)\n",
    "                if self._activation is not None:\n",
    "                    c = self._activation(c)\n",
    "            output = new_state = u * state + (1 - u) * c\n",
    "            if self._num_proj is not None:\n",
    "                with tf.variable_scope(\"projection\"):\n",
    "                    w = tf.get_variable('w', shape=(self._num_units, self._num_proj))\n",
    "                    batch_size = inputs.get_shape()[0].value\n",
    "                    output = tf.reshape(new_state, shape=(-1, self._num_units))\n",
    "                    output = tf.reshape(tf.matmul(output, w), shape=(batch_size, self.output_size))\n",
    "        return output, new_state\n",
    "\n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)\n",
    "\n",
    "    def _fc(self, inputs, state, output_size, bias_start=0.0):\n",
    "        dtype = inputs.dtype\n",
    "        batch_size = inputs.get_shape()[0].value\n",
    "        inputs = tf.reshape(inputs, (batch_size * self._num_nodes, -1))\n",
    "        state = tf.reshape(state, (batch_size * self._num_nodes, -1))\n",
    "        inputs_and_state = tf.concat([inputs, state], axis=-1)\n",
    "        input_size = inputs_and_state.get_shape()[-1].value\n",
    "        weights = tf.get_variable(\n",
    "            'weights', [input_size, output_size], dtype=dtype,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        value = tf.nn.sigmoid(tf.matmul(inputs_and_state, weights))\n",
    "        biases = tf.get_variable(\"biases\", [output_size], dtype=dtype,\n",
    "                                 initializer=tf.constant_initializer(bias_start, dtype=dtype))\n",
    "        value = tf.nn.bias_add(value, biases)\n",
    "        return value\n",
    "\n",
    "    def _gconv(self, inputs, state, output_size, bias_start=0.0):\n",
    "        \"\"\"Graph convolution between input and the graph matrix.\n",
    "\n",
    "        :param args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "        :param output_size:\n",
    "        :param bias:\n",
    "        :param bias_start:\n",
    "        :param scope:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Reshape input and state to (batch_size, num_nodes, input_dim/state_dim)\n",
    "        batch_size = inputs.get_shape()[0].value\n",
    "        inputs = tf.reshape(inputs, (batch_size, self._num_nodes, -1))\n",
    "        state = tf.reshape(state, (batch_size, self._num_nodes, -1))\n",
    "        inputs_and_state = tf.concat([inputs, state], axis=2)\n",
    "        input_size = inputs_and_state.get_shape()[2].value\n",
    "        dtype = inputs.dtype\n",
    "\n",
    "        x = inputs_and_state\n",
    "        x0 = tf.transpose(x, perm=[1, 2, 0])  # (num_nodes, total_arg_size, batch_size)\n",
    "        x0 = tf.reshape(x0, shape=[self._num_nodes, input_size * batch_size])\n",
    "        x = tf.expand_dims(x0, axis=0)\n",
    "\n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            if self._max_diffusion_step == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for support in self._supports:\n",
    "                    x1 = tf.sparse_tensor_dense_matmul(support, x0)\n",
    "                    x = self._concat(x, x1)\n",
    "\n",
    "                    for k in range(2, self._max_diffusion_step + 1):\n",
    "                        x2 = 2 * tf.sparse_tensor_dense_matmul(support, x1) - x0\n",
    "                        x = self._concat(x, x2)\n",
    "                        x1, x0 = x2, x1\n",
    "\n",
    "            num_matrices = len(self._supports) * self._max_diffusion_step + 1  # Adds for x itself.\n",
    "            x = tf.reshape(x, shape=[num_matrices, self._num_nodes, input_size, batch_size])\n",
    "            x = tf.transpose(x, perm=[3, 1, 2, 0])  # (batch_size, num_nodes, input_size, order)\n",
    "            x = tf.reshape(x, shape=[batch_size * self._num_nodes, input_size * num_matrices])\n",
    "\n",
    "            weights = tf.get_variable(\n",
    "                'weights', [input_size * num_matrices, output_size], dtype=dtype,\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            x = tf.matmul(x, weights)  # (batch_size * self._num_nodes, output_size)\n",
    "\n",
    "            biases = tf.get_variable(\"biases\", [output_size], dtype=dtype,\n",
    "                                     initializer=tf.constant_initializer(bias_start, dtype=dtype))\n",
    "            x = tf.nn.bias_add(x, biases)\n",
    "        # Reshape res back to 2D: (batch_size, num_node, state_dim) -> (batch_size, num_node * state_dim)\n",
    "        return tf.reshape(x, [batch_size, self._num_nodes * output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-51c7206a84e1>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-51c7206a84e1>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    sess.run(cell._gconv(input, state=,output_size=5))\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "adj_matrix = np.array([[1,0,1],[0,1,0],[1,0,1]])\n",
    "#print(str(adj_matrix.get_shape()))\n",
    "input = tf.convert_to_tensor([[[1,2,3],[4,3,2]],[[2,3,2],[1,2,1]]])\n",
    "state = tf.zero\n",
    "\n",
    "cell = DCGRUCell(5, adj_matrix, 2, 2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(cell._gconv(input, state=,output_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class DCGRUCell_py(nn.RNNCellBase):\n",
    "    \"\"\"Graph Convolution Gated Recurrent Unit cell.\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, num_units, adj_mx, max_diffusion_step, num_nodes, num_proj=None,\n",
    "                 activation=tf.nn.tanh, reuse=None, filter_type=\"laplacian\", use_gc_for_ru=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param num_units:\n",
    "        :param adj_mx:\n",
    "        :param max_diffusion_step:\n",
    "        :param num_nodes:\n",
    "        :param input_size:\n",
    "        :param num_proj:\n",
    "        :param activation:\n",
    "        :param reuse:\n",
    "        :param filter_type: \"laplacian\", \"random_walk\", \"dual_random_walk\".\n",
    "        :param use_gc_for_ru: whether to use Graph convolution to calculate the reset and update gates.\n",
    "        \"\"\"\n",
    "        super(DCGRUCell, self).__init__(_reuse=reuse)\n",
    "        self._activation = activation\n",
    "        self._num_nodes = num_nodes\n",
    "        self._num_proj = num_proj\n",
    "        self._num_units = num_units\n",
    "        self._max_diffusion_step = max_diffusion_step\n",
    "        self._supports = []\n",
    "        self._use_gc_for_ru = use_gc_for_ru\n",
    "        supports = []\n",
    "        if filter_type == \"laplacian\":\n",
    "            supports.append(utils.calculate_scaled_laplacian(adj_mx, lambda_max=None))\n",
    "        elif filter_type == \"random_walk\":\n",
    "            supports.append(utils.calculate_random_walk_matrix(adj_mx).T)\n",
    "        elif filter_type == \"dual_random_walk\":\n",
    "            supports.append(utils.calculate_random_walk_matrix(adj_mx).T)\n",
    "            supports.append(utils.calculate_random_walk_matrix(adj_mx.T).T)\n",
    "        else:\n",
    "            supports.append(utils.calculate_scaled_laplacian(adj_mx))\n",
    "        for support in supports:\n",
    "            self._supports.append(self._build_sparse_matrix(support))\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_nodes * self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        output_size = self._num_nodes * self._num_units\n",
    "        if self._num_proj is not None:\n",
    "            output_size = self._num_nodes * self._num_proj\n",
    "        return output_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Gated recurrent unit (GRU) with Graph Convolution.\n",
    "        :param inputs: (B, num_nodes * input_dim)\n",
    "\n",
    "        :return\n",
    "        - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n",
    "        - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
    "            the arity and shapes of `state`\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(scope or \"dcgru_cell\"):\n",
    "            with tf.variable_scope(\"gates\"):  # Reset gate and update gate.\n",
    "                output_size = 2 * self._num_units\n",
    "                # We start with bias of 1.0 to not reset and not update.\n",
    "                if self._use_gc_for_ru:\n",
    "                    fn = self._gconv\n",
    "                else:\n",
    "                    fn = self._fc\n",
    "                value = tf.nn.sigmoid(fn(inputs, state, output_size, bias_start=1.0))\n",
    "                value = tf.reshape(value, (-1, self._num_nodes, output_size))\n",
    "                r, u = tf.split(value=value, num_or_size_splits=2, axis=-1)\n",
    "                r = tf.reshape(r, (-1, self._num_nodes * self._num_units))\n",
    "                u = tf.reshape(u, (-1, self._num_nodes * self._num_units))\n",
    "            with tf.variable_scope(\"candidate\"):\n",
    "                c = self._gconv(inputs, r * state, self._num_units)\n",
    "                if self._activation is not None:\n",
    "                    c = self._activation(c)\n",
    "            output = new_state = u * state + (1 - u) * c\n",
    "            if self._num_proj is not None:\n",
    "                with tf.variable_scope(\"projection\"):\n",
    "                    w = tf.get_variable('w', shape=(self._num_units, self._num_proj))\n",
    "                    batch_size = inputs.get_shape()[0].value\n",
    "                    output = tf.reshape(new_state, shape=(-1, self._num_units))\n",
    "                    output = tf.reshape(tf.matmul(output, w), shape=(batch_size, self.output_size))\n",
    "        return output, new_state\n",
    "\n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)\n",
    "\n",
    "    def _fc(self, inputs, state, output_size, bias_start=0.0):\n",
    "        dtype = inputs.dtype\n",
    "        batch_size = inputs.get_shape()[0].value\n",
    "        inputs = tf.reshape(inputs, (batch_size * self._num_nodes, -1))\n",
    "        state = tf.reshape(state, (batch_size * self._num_nodes, -1))\n",
    "        inputs_and_state = tf.concat([inputs, state], axis=-1)\n",
    "        input_size = inputs_and_state.get_shape()[-1].value\n",
    "        weights = tf.get_variable(\n",
    "            'weights', [input_size, output_size], dtype=dtype,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        value = tf.nn.sigmoid(tf.matmul(inputs_and_state, weights))\n",
    "        biases = tf.get_variable(\"biases\", [output_size], dtype=dtype,\n",
    "                                 initializer=tf.constant_initializer(bias_start, dtype=dtype))\n",
    "        value = tf.nn.bias_add(value, biases)\n",
    "        return value\n",
    "\n",
    "    def _gconv(self, inputs, state, output_size, bias_start=0.0):\n",
    "        \"\"\"Graph convolution between input and the graph matrix.\n",
    "\n",
    "        :param args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "        :param output_size:\n",
    "        :param bias:\n",
    "        :param bias_start:\n",
    "        :param scope:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Reshape input and state to (batch_size, num_nodes, input_dim/state_dim)\n",
    "        batch_size = inputs.get_shape()[0].value\n",
    "        inputs = tf.reshape(inputs, (batch_size, self._num_nodes, -1))\n",
    "        state = tf.reshape(state, (batch_size, self._num_nodes, -1))\n",
    "        inputs_and_state = tf.concat([inputs, state], axis=2)\n",
    "        input_size = inputs_and_state.get_shape()[2].value\n",
    "        dtype = inputs.dtype\n",
    "\n",
    "        x = inputs_and_state\n",
    "        x0 = tf.transpose(x, perm=[1, 2, 0])  # (num_nodes, total_arg_size, batch_size)\n",
    "        x0 = tf.reshape(x0, shape=[self._num_nodes, input_size * batch_size])\n",
    "        x = tf.expand_dims(x0, axis=0)\n",
    "\n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            if self._max_diffusion_step == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for support in self._supports:\n",
    "                    x1 = tf.sparse_tensor_dense_matmul(support, x0)\n",
    "                    x = self._concat(x, x1)\n",
    "\n",
    "                    for k in range(2, self._max_diffusion_step + 1):\n",
    "                        x2 = 2 * tf.sparse_tensor_dense_matmul(support, x1) - x0\n",
    "                        x = self._concat(x, x2)\n",
    "                        x1, x0 = x2, x1\n",
    "\n",
    "            num_matrices = len(self._supports) * self._max_diffusion_step + 1  # Adds for x itself.\n",
    "            x = tf.reshape(x, shape=[num_matrices, self._num_nodes, input_size, batch_size])\n",
    "            x = tf.transpose(x, perm=[3, 1, 2, 0])  # (batch_size, num_nodes, input_size, order)\n",
    "            x = tf.reshape(x, shape=[batch_size * self._num_nodes, input_size * num_matrices])\n",
    "\n",
    "            weights = tf.get_variable(\n",
    "                'weights', [input_size * num_matrices, output_size], dtype=dtype,\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            x = tf.matmul(x, weights)  # (batch_size * self._num_nodes, output_size)\n",
    "\n",
    "            biases = tf.get_variable(\"biases\", [output_size], dtype=dtype,\n",
    "                                     initializer=tf.constant_initializer(bias_start, dtype=dtype))\n",
    "            x = tf.nn.bias_add(x, biases)\n",
    "        # Reshape res back to 2D: (batch_size, num_node, state_dim) -> (batch_size, num_node * state_dim)\n",
    "        return tf.reshape(x, [batch_size, self._num_nodes * output_size])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
